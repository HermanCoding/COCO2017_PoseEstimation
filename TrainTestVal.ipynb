{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T10:03:25.362460900Z",
     "start_time": "2023-06-28T10:03:21.817432600Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import ultralytics\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-28T10:03:25.390388500Z",
     "start_time": "2023-06-28T10:03:25.362460900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.123  Python-3.11.2 torch-2.0.1+cpu CPU\n",
      "Setup complete  (8 CPUs, 15.8 GB RAM, 217.2/475.7 GB disk)\n"
     ]
    }
   ],
   "source": [
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T10:03:25.514774900Z",
     "start_time": "2023-06-28T10:03:25.418690800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8n-pose summary: 250 layers, 3295470 parameters, 0 gradients\n"
     ]
    },
    {
     "data": {
      "text/plain": "(250, 3295470, 0, 0.0)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a model\n",
    "model = YOLO(\"yolov8n-pose.pt\")  # This model is pretrained. To setup a empty model use : model = YOLO('yolov8n.yaml')\n",
    "model.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "The COCO8-Pose dataset is a tiny dataset ment for testing pipelines for errors or as a check before training larger datasets.\n",
    "It is a good dataset to show how easy it is to train a dataset using ultralytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T10:03:25.515776100Z",
     "start_time": "2023-06-28T10:03:25.475035700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# model.train(data='coco8-pose.yaml', epochs=100, imgsz=640) # TODO glöm inte ta bort kommentaren!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate\n",
    "The re-trained YOLOv8n-pose model accuracy on the COCO8-pose dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T10:03:25.515776100Z",
     "start_time": "2023-06-28T10:03:25.480542600Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = YOLO('runs/pose/train/weights/best.pt')  # load a custom model # TODO glöm inte ta bort kommentaren!!!\n",
    "\n",
    "# Validate the model\n",
    "# metrics = model.val()  # no arguments needed, dataset and settings remembered"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T10:03:26.022308Z",
     "start_time": "2023-06-28T10:03:25.485225600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\dherm\\python\\AI2_exam_assignment\\bus.jpg: 640x480 3 persons, 134.2ms\n",
      "Speed: 3.0ms preprocess, 134.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ultralytics.yolo.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.yolo.engine.results.Boxes object\n",
      "keypoints: ultralytics.yolo.engine.results.Keypoints object\n",
      "keys: ['boxes', 'keypoints']\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "orig_img: array([[[122, 148, 172],\n",
      "        [120, 146, 170],\n",
      "        [125, 153, 177],\n",
      "        ...,\n",
      "        [157, 170, 184],\n",
      "        [158, 171, 185],\n",
      "        [158, 171, 185]],\n",
      "\n",
      "       [[127, 153, 177],\n",
      "        [124, 150, 174],\n",
      "        [127, 155, 179],\n",
      "        ...,\n",
      "        [158, 171, 185],\n",
      "        [159, 172, 186],\n",
      "        [159, 172, 186]],\n",
      "\n",
      "       [[128, 154, 178],\n",
      "        [126, 152, 176],\n",
      "        [126, 154, 178],\n",
      "        ...,\n",
      "        [158, 171, 185],\n",
      "        [158, 171, 185],\n",
      "        [158, 171, 185]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[185, 185, 191],\n",
      "        [182, 182, 188],\n",
      "        [179, 179, 185],\n",
      "        ...,\n",
      "        [114, 107, 112],\n",
      "        [115, 105, 111],\n",
      "        [116, 106, 112]],\n",
      "\n",
      "       [[157, 157, 163],\n",
      "        [180, 180, 186],\n",
      "        [185, 186, 190],\n",
      "        ...,\n",
      "        [107,  97, 103],\n",
      "        [102,  92,  98],\n",
      "        [108,  98, 104]],\n",
      "\n",
      "       [[112, 112, 118],\n",
      "        [160, 160, 166],\n",
      "        [169, 170, 174],\n",
      "        ...,\n",
      "        [ 99,  89,  95],\n",
      "        [ 96,  86,  92],\n",
      "        [102,  92,  98]]], dtype=uint8)\n",
      "orig_shape: (1080, 810)\n",
      "path: 'C:\\\\Users\\\\dherm\\\\python\\\\AI2_exam_assignment\\\\bus.jpg'\n",
      "probs: None\n",
      "save_dir: None\n",
      "speed: {'preprocess': 3.002166748046875, 'inference': 134.2451572418213, 'postprocess': 2.003192901611328}]\n"
     ]
    }
   ],
   "source": [
    "# Predict with the model\n",
    "predict = model.predict('bus.jpg', conf=0.75)\n",
    "# predict on an image\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\dherm\\python\\AI2_exam_assignment\\bus.jpg: 640x480 3 persons, 101.1ms\n",
      "Speed: 3.5ms preprocess, 101.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "# Show the image with keypoint names\n",
    "\n",
    "img = cv2.imread('https://ultralytics.com/images/bus.jpg')\n",
    "\n",
    "results = model('https://ultralytics.com/images/bus.jpg', conf=0.75)[0]\n",
    "\n",
    "for result in results:\n",
    "    for keypoint_index, keypoint in enumerate(result.keypoints.data[0].tolist()):\n",
    "        cv2.putText(img, str(keypoint_index), (int(keypoint[0]), int(keypoint[1])),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "cv2.imshow('Results', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-28T10:03:29.782222100Z",
     "start_time": "2023-06-28T10:03:26.025305900Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
